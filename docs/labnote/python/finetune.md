# Finetune

注意力机制解决了长距离依赖问题和梯度消失问题，关注重点，提高了并行计算的效率

统计语言模型

神经网络语言模型

| 阶段                      | 定义与历史背景                                               | 里程碑成果                                                   | 限制                                             |
| ------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------ |
| 统计语言模型              | 初期的语言模型，依赖于统计分布和频率来预测单词。起源于信息论和早期计算机科学。 | 1. 统计机器翻译<br>2. n-gram模型和HMM<br>3. 最大熵模型       | 1. 难以捕捉长期依赖性<br>2. 需要复杂的特征工程   |
| 神经网络语言模型          | 神经网络在捕捉复杂模式方面的优势使其成为语言模型的一个进步。包括RNN和LSTM在内的模型被用于捕捉序列数据中的依赖关系。 | 1. 神经网络语言模型（NNLM）<br>2. RNN处理序列数据<br>3. LSTM在语言模型中的应用 | 1. 计算效率和扩展性有限<br>2. 梯度消失或爆炸问题 |
| 基于Transformer大语言模型 | Transformer架构通过自注意力机制改进了长距离依赖问题的处理，并提高了并行化处理的效率。 | 1. Transformer架构<br>2. BERT 模型<br>3. GPT 模型家族        | 1. 计算资源消耗<br>2. 可能放大数据偏见           |
