# Finetune

注意力机制解决了长距离依赖问题和梯度消失问题，关注重点，提高了并行计算的效率

统计语言模型

神经网络语言模型

| 阶段                      | 定义与历史背景                                               | 里程碑成果                                                   | 限制                                             |
| ------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------ |
| 统计语言模型              | 初期的语言模型，依赖于统计分布和频率来预测单词。起源于信息论和早期计算机科学。 | 1. 统计机器翻译<br>2. n-gram模型和HMM<br>3. 最大熵模型       | 1. 难以捕捉长期依赖性<br>2. 需要复杂的特征工程   |
| 神经网络语言模型          | 神经网络在捕捉复杂模式方面的优势使其成为语言模型的一个进步。包括RNN和LSTM在内的模型被用于捕捉序列数据中的依赖关系。 | 1. 神经网络语言模型（NNLM）<br>2. RNN处理序列数据<br>3. LSTM在语言模型中的应用 | 1. 计算效率和扩展性有限<br>2. 梯度消失或爆炸问题 |
| 基于Transformer大语言模型 | Transformer架构通过自注意力机制改进了长距离依赖问题的处理，并提高了并行化处理的效率。 | 1. Transformer架构<br>2. BERT 模型<br>3. GPT 模型家族        | 1. 计算资源消耗<br>2. 可能放大数据偏见           |

| 特性     | BERT                                                 | GPT                                    |
| -------- | ---------------------------------------------------- | -------------------------------------- |
| 训练方式 | 自编码（Autoencoding）                               | 自回归（Autoregressive）               |
| 预测目标 | 给定上下文，预测其中的一个或多个缺失单词             | 在给定前面的单词时，预测下一个单词     |
| 输入处理 | 双向，可以同时考虑一个词的左右上下文                 | 单向（从左到右或者从右到左）           |
| 适用场景 | 适合理解上下文，有助于信息提取、问答系统、情感分析等 | 适合生成式任务，如文章生成、诗歌创作等 |
| 架构     | 基于Transformer的编码器                              | 基于Transformer的解码器                |
| 语言模型 | 判别式（Discriminative）                             | 生成式（Generative）                   |
| 优点     | 对上下文理解能力较强                                 | 预测的连贯性较强                       |
| 缺点     | 生成的文本连贯性较弱                                 | 对上下文理解能力相对较弱               |



## BERT vs GPT 共识

| 特性       | 描述                                                         |
| ---------- | ------------------------------------------------------------ |
| 模型架构   | Transformer                                                  |
| 数据预处理 | 都需要对数据进行Tokenization，一般使用词片方法（Subword Tokenization） |
| 模型训练   | 均使用了大量的无标签数据进行预训练                           |
| 任务迁移   | 都可以通过Fine-tuning方式进行任务迁移                        |
| 训练目标   | 都试图通过预训练理解语言的一般模式，如语法、语义、上下文关系等 |
| 多语言支持 | 均支持多语言模型训练                                         |
