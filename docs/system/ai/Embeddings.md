# Embeddings

> Embeddings 是一种将高维数据（如单词、句子、图像等）映射到低维连续向量空间的技术，常用于自然语言处理、计算机视觉等领域。它将复杂的结构或对象转换为固定维度的向量表示，使得这些对象在新的空间中保留了其语义或结构上的特征

### 前言

> Input: token
>
> Output: 高纬度的向量

在此之前，我只是单纯的以为 Embedding 技术只是将语意投射到一个高纬度的矩阵空间中，网上大部分的解释也都是片面的，经过系统的学习，我有了一些充分的认识

## Token 数字化

什么是 Token ？在语言模型中是指文本中最基本的**处理单元**，具有独立的语意，分词的粒度（词、字符）由分词的算法决定

为什么要数字化？ 人类的语言非常抽象、复杂，为了让机器可以对语言进行**运算**，比如 “国王” - “男人” + “女人” = “女王”

我们需要有个黑盒子，可以把 曹冲称的象转化成石头，把石头转化成象，对石头进行切割、平移方便运算，来实现 “国王” - “男人” + “女人” = “女王” 的语意关系，这俩个过程就对应着**编码**和**解码**的过程

我们对 Token 分词数字话有俩种极端情况，**分词器**：把所有的 token 投射到一个一维空间（一条直线），**one-hot**：独热编码把每个 token 都投射到一个纬度

> 在分词器中，苹果、梨、橘子可能是一条直线的 1、2、3，而在 one-hot 中 可能是 (1,0,0)、(0,1,0)、(0,0,1)
>
> 纬度可以抽象理解为 一个矩阵有多少列，有多少纬度就有多少特征去描述这个语意，比如 0.8的红色 + 0.9的圆 + 1的水果 = 苹果，前面的参数就是矩阵对应列的值，描述就是特征，在 one-hot 中则是 1的苹果 = 苹果

分词器和 one-hot 都能将 Token 与它们的纬度中 一一对应，分词器由于在一维（一条直线上）上表示，信息密度过大，几乎无法实现复杂的运算，比如把  “国王” - “男人” + “女人” = “女王” 中的 Token 按照分词器数字化后，可能会呈现 4 - 2 + 1 = 3，女人是1，男人是2，女王是3，国王是4，但 “女人” + “男人” = “人类” 出现冲突，而且无法保证类似 “苹果” 是“水果”还是“手机”的语意。one-hot 有多少 Token 就有多少纬度，信息密度过低，所有 Token 相互之间都是正交的，且长度都是1（标准正交基），无法使用长度的关系，很难体现 Token 之间的联系。分词器 Token 之间的联系体现在长度上，而 one-hot Token 之间的联系体现在纬度上，都无法满足我们的要求。

显然我们需要纬度没有那么高，且有长度的纬度空间中，去协助编码和解码的过程，这个空间就是我们常说的**潜空间**，那么如何找到这个潜空间呢？

我们可以通过将分词器升纬，或者 one-hot 降维，而在实践中，我们往往会把 one-hot 降纬，这一步就叫 Embeddings
