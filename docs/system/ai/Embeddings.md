# Embeddings

> Embeddings 是一种将高维数据（如单词、句子、图像等）映射到低维连续向量空间的技术，常用于自然语言处理、计算机视觉等领域。它将复杂的结构或对象转换为固定维度的向量表示，使得这些对象在新的空间中保留了其语义或结构上的特征
>
> 用途：自然语言处理（NLP），推荐系统，计算机视觉，图数据

### 前言

> Input: token
>
> Output: 高维度的向量

在此之前，我只是单纯的以为 Embedding 技术只是将语意投射到一个高维度的矩阵空间中，网上大部分的解释也都是片面的，经过系统的学习，我有了一些充分的认识

## Token 数字化

什么是 Token ？在语言模型中是指文本中最基本的**处理单元**，具有独立的语意，分词的粒度（词、字符）由分词的算法决定

为什么要数字化？ 人类的语言非常抽象、复杂，为了让机器可以对语言进行**运算**，比如 “国王” - “男人” + “女人” = “女王”

我们需要有个黑盒子，可以把 曹冲称的象转化成石头，把石头转化成象，对石头进行切割、平移方便运算，来实现 “国王” - “男人” + “女人” = “女王” 的语意关系，这俩个过程就对应着**编码**和**解码**的过程

我们对 Token 分词数字话有俩种极端情况，**分词器**：把所有的 token 投射到一个一维空间（一条直线），**one-hot**：独热编码把每个 token 都投射到一个维度

> 在分词器中，苹果、梨、橘子可能是一条直线的 1、2、3，而在 one-hot 中 可能是 (1,0,0)、(0,1,0)、(0,0,1)
>
> 维度可以抽象理解为 一个矩阵有多少列，有多少维度就有多少特征去描述这个语意，比如 0.8的红色 + 0.9的圆 + 1的水果 = 苹果，前面的参数就是矩阵对应列的值，描述就是特征，在 one-hot 中则是 1的苹果 = 苹果

分词器和 one-hot 都能将 Token 与它们的维度中 一一对应，分词器由于在一维（一条直线上）上表示，信息密度过大，几乎无法实现复杂的运算，比如把  “国王” - “男人” + “女人” = “女王” 中的 Token 按照分词器数字化后，可能会呈现 4 - 2 + 1 = 3，女人是1，男人是2，女王是3，国王是4，但 “女人” + “男人” = “人类” 出现冲突，而且无法保证类似 “苹果” 是“水果”还是“手机”的语意。one-hot 有多少 Token 就有多少维度，信息密度过低，所有 Token 相互之间都是正交的，且长度都是1（标准正交基），无法使用长度的关系，很难体现 Token 之间的联系。分词器 Token 之间的联系体现在长度上，而 one-hot Token 之间的联系体现在维度上，都无法满足我们的要求。

显然我们需要维度没有那么高，且有长度的维度空间中，去协助编码和解码的过程，这个空间就是我们常说的**潜空间**，那么如何找到这个潜空间呢？

我们可以通过将分词器升维，或者 one-hot 降维，而在实践中，我们往往会把 one-hot 降维，这一步就叫 Embeddings，Word2Vec 是其中的一个处理自然语言的用法

## Word2Vec

> Word2Vec 是一个机器学习的方法，目标是得到一个嵌入矩阵，而不是目标的结果

我们可以使用机器学习来找实现降维的嵌入矩阵，在 Word2Vec 中有两种算法：

CBOW：准备奇数个 Token，把中间的拿掉，剩下的与同一个嵌入矩阵相乘，变成潜空间的词向量，再把四个向量加在一起合成一个向量，再解码，损失函数定量去看这个和向量解码后的 Token 与去掉的那个 Token 比较，修改参数

> 为什么剩下的词向量加在一起是去掉的词向量呢？按照物理的受力分析，这几个 Token（上下文）都应该往一个地方使劲，相当于完形填空，需要根据上下文来确定语意。比如 “这是一个__苹果”，可以填红、甜等不同的词，这没关系，他们在语意上接近，起码都是形容词，这种语意的训练是根据整个语言环境所决定的

Skip-gram：与 CBOW 相反，已知一个 Token，根据它的词向量去求上下文对应 Token 的分量，预测上下文
