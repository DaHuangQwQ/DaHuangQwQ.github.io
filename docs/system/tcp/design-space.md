# 03 How Design Space

如何设计拥塞控制

## 选择

### 中心化还是分布式

原则上来说，网络资源分配的第一个设计决策就是：采取集中式还是分布式方法？实际上，互联网的规模以及连接到互联网的不同组织的自治性，必然会导致采用分布式方法，原因是

* 互联网的拥塞控制方法是在其数百万个主机和路由器之间分布式实施的，我们可以合理地将它们视为以协作的方式来实现全局最优解。从这个角度看，存在一个共享的目标函数，所有元素都在执行一个分布式算法来优化该目标函数。本书中描述的各种机制只是定义了不同的目标函数，但是一直存在的挑战是，当多种不同的机制部署之后，如何考虑这些目标函数之间的竞争。
* 虽然对于整个互联网来说，采用中心化方法不太可行，但对于一个有限的范围来说，这种方法可能却更加合适。例如，一个逻辑上的中心化控制器可以收集关于网络链路和交换机状态的信息，计算出全局最优的资源分配方案，然后向终端用户建议（甚至限制）每个用户可用的容量。这种方法肯定受限于中心控制器响应网络变化的时间快慢，但它已经成功应用于 B4 和 SWAN 这一类粗粒度资源分配的流量工程机制中。在粗粒度流量工程决策和细粒度拥塞控制决策之间，并没有明显的界限，但是对可用选项保持开放的态度是有好处的。

中心化控制在数据中心内部已经被有效的应用起来。数据中心是拥塞控制问题的一个有趣的环境。首先，它的 RTT 非常低。其次，在许多情况下，可以将数据中心视为一个干净的环境，因为受控所以不用考虑各种现存的拥塞控制算法，从而使得新的拥塞控制算法实施可能性更大。Fastpass 就是这样一种集中式方法的良好例证，它是 MIT 和 Facebook 研究人员合作开发的。

### 路由器还是主机为中心

既然资源分配是分布式的，那下一个问题就是：应该在网络内部（路由器或交换机）还是在网络边缘（主机）实现拥塞控制机制。

1. 以路由器为中心。路由器可以允许主机来向自己预留容量，然后确保每个流的数据包基于该容量相应地被传送。例如，他们可以通过实施一个信号协议以及Fair Queue来做到这一点，只在当有足够容量时才接受新的数据流，同时也管理主机，以确保它们的数据流只使用给它们预留的资源。这对应于一个基于预留的方法，其中的网络可以保证 QoS。但我们认为这个内容超出了本书的讨论范围。
2. 是以主机为中心。路由器不提供任何保证，也不提供关于可用容量的明确反馈（当其缓冲区满时会静默丢包），而是由主机负责观察网络状况（例如，他们成功通过网络发送了多少个数据包）并相应地调整其行为。
3. 介于这两个方案中间的位置。路由器可以采取更积极的措施来帮助终端主机完成拥塞控制的任务，但这不是通过预留容量和缓冲区空间来实现。当路由器的缓冲区满时，它会向终端主机发送一个反馈。

一般来说，以主机为中心的拥塞控制算法实现在位于传输层的TCP中，或者一些其他模仿TCP的传输层协议，例如_DCCP(datagram congestion control protocol)_ 或 _QUIC（一种为基于HTTP的应用设计的相对较新的传输协议）_。然而，也可以在应用程序本身中实现拥塞控制。_DASH_（_Dynamic Adaptive Streaming over HTTP_）就是一个例子，尽管它通常被认为是传输层（因为它是基于TCP运行的）和应用层的拥塞控制的组合。通过测量的网络性能，视频流服务器可以向客户端发送不同的视频编码，并因此改变HTTP流的速率。实际上，TCP会尝试找到数据流的稳定带宽，然后应用程序调整其发送速率，以完全利用该稳定带宽，同时不发送超过当前网络条件下可承受的数据量。拥塞控制的主要责任属于TCP，但应用程序旨在保持管道充满，同时也维护良好的用户体验。

### 基于窗口还是基于速率

在确定了以主机为中心的方法后，下一个实现选择是基于_窗口_ 的还是基于_速率 。_TCP 使用基于窗口机制来实现流控制，因此对于 TCP 拥塞控制的设计决策似乎很明显（即使用相同的方式）。事实上，第四章描述的拥塞控制机制围绕着一种计算 _拥塞窗口_ 的算法展开_，_在传输过程中，发送方的发送速率将受限于以下两者中较小的一个：接收端宣告的的流控窗口或发送端自己计算出的拥塞控制窗口。

但其实也可以通过计算出网络能承受的速率，然后调整传输速度来实现拥塞控制。真实的传输速率是在某个时间段（如RTT）内传输的字节数。我们在这里同时提出基于速率和窗口的方法，是因为在不同的场景适用不同的方法。例如，基于速率的方法更适合于多媒体应用程序，这类程序以某种平均速率生成数据，并且需要一定量最小的吞吐量才能有效工作。例如，一个视频编解码器可能以平均速率1 Mbps生成视频，峰值速率达到2 Mbps。

通过基于资源预留的方法来实现不同QoS等级的系统中（注，即以路由器为中心的极端方法），采用基于速率的拥塞控制方法是合理的选择。但即便在像互联网这样的尽力而为传输模型中，也可以实现一种自适应的基于速率的拥塞控制机制，该机制会通知应用程序何时需要通过例如调整其编解码器，来调整其传输速率。这就是 TCP-friendly rate control （TFRC） 的核心思想，它将TCP 预防拥塞的概念扩展到那些需要以特定速率发送数据包的应用程序中（例如，以特定质量级别的视频编解码产生的比特率）。TFRC通常与RTP一起使用，RTP是为实时应用设计的传输协议。我们将在第7章看到这类机制的示例。

最后，TCP拥塞控制领域一个最新的进展是BBR（Bottleneck Bandwidth and RTT），它结合了基于窗口的拥塞控制和基于速率的拥塞控制，以限制网络内队列的堆积。我们将在第5章详细介绍这种方法。

### 基于控制还是基于规避

我们最后关注的实现选择，相对来说有点微妙。这里的挑战在于终端主机，它需要基于反馈和观察来计算当前网络有多少可用容量，并以此调整其发送速率。大体上来说，有两种策略：一种是激进的方法，主机会故意以造成数据包丢失的速率发送数据包，然后真正丢包时作出响应；另一种是保守的方法，主机试图探测路由器队列堆积的开始阶段，并在队列真正溢出到丢包之前放慢速度。我们将第一种机制称为基于控制（control-based），第二种机制称为基于规避（avoidance-based）。

它们的区别经常被忽视，并且“拥塞控制”一词通常被泛泛地用来指代两者，但我们需要知道它们有着重要的区别。不过，必须承认，如果区别不是很关键的时候，我们还是会用“拥塞控制”一词来泛指两种方式。

同时需要注意的是，我们称之为“基于控制”的和“基于避免”的方法，有时也会分别被称为 _基于丢包（loss-based）_和 _基于延时（delay-based）_，这是根据它们使用哪种标准来作为调整拥塞窗口的信号。前者在检测到丢包时调整窗口，而后者在检测到延迟变化时调整窗口。从这个角度看，接下来四个章节介绍的每种算法实际上都是以某种方式改进了这些信号的准确性。

## 评判标准

### 有效性

在评估拥塞控制机制的有效性时，可以从网络的两个主要指标开始，它们是：吞吐量和延迟。显然，我们希望吞吐量尽可能高，延迟尽可能低。不幸的是，这些目标之间可能彼此矛盾。一种增加吞吐量的方法是让尽可能多的数据包进入网络，以此来驱动所有链路的利用率达到100%。我们这样做是为了避免链路空闲，因为空闲的链路会降低吞吐量。但这种策略的问题在于，增加网络中的数据包数量的同时也会增加每个路由器的队列长度。这样的持续队列累积意味着数据包在网络中被延迟了，更坏的情况下数据包会被丢弃。在网络中间丢包不仅会影响延时，还会降低吞吐量，因为数据包未能成功送到目的地会导致上游链路带宽被浪费。

> 我们有时使用goodput，而不是吞吐量（throughput）来强调我们关心的是成功通过网络送到接收端的数据，而不仅仅是发送端发出的数据。

系统吞吐量与延迟的比例是评估拥塞控制机制（即网络资源的分配方案）效果的一个通用指标。这个比例有时被称为系统的_功率（Power）：_

$$Power = Throughput / Delay$$

直观地说，这里Power值是你给网络系统施加的负载的函数，而我们的目标就是最大化Power。施加的负载由拥塞控制机制（即网络资源的分配方案）决定。图11 展示了一个有代表性的功率曲线，理想情况下，资源分配机制会保持在这个曲线的顶点。在峰值左侧，机制过于保守；也就是说，它没有允许发送足够多的数据包来保持链路繁忙。在峰值右侧，则是允许进入网络的数据包太多，以至于(a)由于队列导致的延迟增加（分母）远大于任何小的吞吐量的增加，或者(b)吞吐量（分子）由于数据包被丢弃而开始下降。

此外，我们还需要关注系统在高负荷运行时的情况——即图11中曲线的右端。理想情况下，我们希望避免系统吞吐量接近零的情况。我们的目标是使机制保持稳定---即使当前负载很高，数据包还是能通过网络传递。如果一种机制不能在高负载下稳定运行，那么网络将会遭受拥塞崩溃。

值得注意的是，尽管“持久队列累积”和“拥塞崩溃”都需要避免，但没有确切定义来界定网络何时受到这两者的影响，它们都是对算法行为的主观判断。最终，延迟和吞吐量还是唯二重要的性能指标。

### 公平性

有效的网络资源利用率并不是判定资源分配方案的唯一标准。我们还必须考虑公平性问题。然而，当我们尝试定义何为公平的资源分配时，我们很快就会进入模糊不清的领域。例如，基于预留的资源分配方案提供了一种明确的受控的不公平分配方案。通过这样的方案，我们可以使用通过预留来使视频流在某个链接上获得1 Mbps的速率，而文件传输在同一链接上仅获得10 kbps的速率。

```
//todo 公平性指标
```



有效的网络资源利用率并不是判定资源分配方案的唯一标准。我们还必须考虑公平性问题。然而，当我们尝试定义何为公平的资源分配时，我们很快就会进入模糊不清的领域。例如，基于预留的资源分配方案提供了一种明确的受控的不公平分配方案。通过这样的方案，我们可以使用通过预留来使视频流在某个链接上获得1 Mbps的速率，而文件传输在同一链接上仅获得10 kbps的速率。

## 对比分析

测量任何拥塞控制机制的第一步是单独评估其性能，这包括了：

* 数据流能达到的平均吞吐量（goodput）
* 数据流经历的平均端到端延时（RTT）
* 在一系列操作场景下避免持续队列堆积的机制
* 在一系列操作场景下保持传输稳定的机制
* 数据流可以多公平的获得网络容量的程度（Jain的公平性指数）

第二步不可避免的要比较两种或者多种机制。这是因为互联网的去中心化性质，无法确保整个互联网只采用一种机制，因此必然要比较两个或多个机制。量化单个机制的指标（如吞吐量）的比较容易，难得是如何评估可能共存并同时竞争网络资源的多个拥塞控制机制。

这里的问题不在于特定机制是否对其所有流量公平对待，而在于机制A是否对由机制B管理的流量公平。如果机制A能够获得相对于B的更高的吞吐量，但这种提升是通过更加激进的方式来窃取B的流量的带宽，那么A的提升并不是公平获得的，可能会不被认可。很明显，互联网高度去中心化的拥塞控制方法之所以有效，是因为大量流量以合作的方式响应拥塞，这就为更激进的，并通过牺牲那些使用了公认的、较不激进算法的流量，来提升自己的性能提供了可能。

在过去的30年中，类似的争论在反复的进行，这对新算法的部署提出了更高的要求。即使新算法的全局部署是一个纯正面的行为，但是灰度部署新算法（这是唯一可用的选项）可能会对使用现有算法的流量造成负面影响，从而阻碍了新算法的部署。但是，就算要得出这样的结论，分析过程也需要考虑三个问题，如Ranysha Ware及其同事们（注，上一篇论文）所识别的：

* **Ideal-Driven Goalposting：** 如果用一个公平性阈值用来判断新机制B与当前部署的机制A平等分享瓶颈链路，那么这个指标在实践中过于理想化，尤其A有时对其自身的流量都不一定不公平。
* **Throughput-Centricity:** 如果一个公平性阈值，通过关注机制A所能达到的吞吐，来判断当存在竞争的数据流时，新机制B是否影响A，那么这个阈值又这忽略了其他重要的性能指标，诸如延迟、流完成时间和丢包率。
* **Assumption of Balance:** 机制间的相互作用可能是正面也可能是负面的，但是[Jain 的公平性指标]无法区分这两种情况。从可部署性的角度来看，新机制B占用的带宽比现有机制A更多，还是会留给A更多的带宽，这两种情况是有区别的：前者可能会引起现有的A用户的抱怨，而后者则不会。但是Jain 的公平指数对这两种情况会得出相同的评分。

相较于简单的计算Jain 的公平指数，Ware提倡基于_伤害_ 设立一个阈值，伤害通过吞吐量减少、延迟增加或抖动增加来衡量。直观上，如果使用新机制B的流对使用现有机制A的流造成的伤害量，少于在一个由A管理的数据流对其他A管理的流造成的伤害量，我们可以认为B与A并行部署而不会造成伤害。Ware接着提出了可接受伤害的具体度量标准，这比最初看起来要复杂得多。即使是单一的拥塞控制算法，一个流对另一个流造成的伤害量也依赖于多种因素，如其RTT、开始时间和持续时间。因此，伤害的度量需要考虑到不同流在现有机制下彼此影响的范围，并力求新算法不会带来更糟的结果。