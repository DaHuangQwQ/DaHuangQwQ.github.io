import{_ as a,c as i,a2 as o,o as e}from"./chunks/framework.BQmytedh.js";const u=JSON.parse('{"title":"不停机数据迁移","description":"","frontmatter":{},"headers":[],"relativePath":"labnote/go/不停机数据迁移.md","filePath":"labnote/go/不停机数据迁移.md"}'),t={name:"labnote/go/不停机数据迁移.md"};function r(p,l,n,c,s,d){return e(),i("div",null,l[0]||(l[0]=[o('<h1 id="不停机数据迁移" tabindex="-1">不停机数据迁移 <a class="header-anchor" href="#不停机数据迁移" aria-label="Permalink to &quot;不停机数据迁移&quot;">​</a></h1><blockquote><p>难点是数据始终在变化</p></blockquote><h2 id="不停机数据迁移的四个方案" tabindex="-1">不停机数据迁移的四个方案 <a class="header-anchor" href="#不停机数据迁移的四个方案" aria-label="Permalink to &quot;不停机数据迁移的四个方案&quot;">​</a></h2><ol><li>第一阶段：业务读写源表</li><li>第二阶段：双写阶段，以源表为准</li><li>第三阶段：双写阶段，以目标表为准</li><li>第四阶段：业务读写目标表</li></ol><h2 id="不停机数据迁移的具体步骤" tabindex="-1">不停机数据迁移的具体步骤 <a class="header-anchor" href="#不停机数据迁移的具体步骤" aria-label="Permalink to &quot;不停机数据迁移的具体步骤&quot;">​</a></h2><ol><li><p>初始化目标表结构</p></li><li><p>用源表的数据初始化目标表</p><blockquote><p>mysql指令复制数据：mysqldump</p></blockquote></li><li><p>执行一次校验，并且修复数据，此时用源表数据修复目标表数据</p></li><li><p>业务代码开启双写，此时读源表，并且先写源表，数据以源表为准。</p></li><li><p>开启增量校验和数据修复，保持一段时间</p></li><li><p>切换双写顺序，此时读目标表，并且先写目标表，数据以目标表为准</p></li><li><p>继续保持增量校验和数据修复</p></li><li><p>切换为目标表单写，读写都只操作目标表</p><blockquote><p>双写阶段要保持增量校验，偶尔执行全量校验</p></blockquote></li></ol><h2 id="注意事项" tabindex="-1">注意事项 <a class="header-anchor" href="#注意事项" aria-label="Permalink to &quot;注意事项&quot;">​</a></h2><ol><li>失败了 等校验和恢复程序</li><li>校验的时候，先读从库，如果从库数据不一致，再度主库校验一遍</li><li>为什么用 kafka？ 削峰 异步 解藕， 批量接口优化</li><li>流量录制与重放，复制请求到 老系统和新系统 比较响应（顶尖大厂才可能做出来，面试官不会相信你做出来）</li><li>在 GORM 中，运用装饰器模式来装饰 原表和目标表的 ConnPool，可以实现 双写、读写分离、分库分表等功能</li></ol>',8)]))}const b=a(t,[["render",r]]);export{u as __pageData,b as default};
