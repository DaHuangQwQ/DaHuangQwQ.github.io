import{_ as t,c as e,a2 as r,o as l}from"./chunks/framework.BQmytedh.js";const i="/assets/image-20250115195754940.DH7qGp9I.png",o="/assets/image-20250116203830055.BvvABoqf.png",m=JSON.parse('{"title":"神经网络","description":"","frontmatter":{},"headers":[],"relativePath":"system/ai/ANN.md","filePath":"system/ai/ANN.md"}'),p={name:"system/ai/ANN.md"};function h(s,a,n,$,d,c){return l(),e("div",null,a[0]||(a[0]=[r('<h3 id="前言" tabindex="-1">前言 <a class="header-anchor" href="#前言" aria-label="Permalink to &quot;前言&quot;">​</a></h3><p>神经网络是模拟人的神经系统，神经系统的基本单元是神经元，神经网络的基本单元是感知机</p><h1 id="神经网络" tabindex="-1">神经网络 <a class="header-anchor" href="#神经网络" aria-label="Permalink to &quot;神经网络&quot;">​</a></h1><h2 id="感知机-perceptron" tabindex="-1">感知机（Perceptron） <a class="header-anchor" href="#感知机-perceptron" aria-label="Permalink to &quot;感知机（Perceptron）&quot;">​</a></h2><p><strong>前言</strong></p><p>感知机就是一个线性函数 + 激活函数</p><p>马文明斯基在 1969 年就提出了感知机的缺陷，让神经网络停滞了 30 年，让我们看看如今是如何解决的感知机的缺陷</p><h3 id="感知机" tabindex="-1">感知机 <a class="header-anchor" href="#感知机" aria-label="Permalink to &quot;感知机&quot;">​</a></h3><p>处理分类问题 二分问题</p><p>加权和 $$ z = \\mathbf{w}^T \\mathbf{x} + b $$ 激活函数 $$ a = f(z) $$ 输出值 $$ y = f(\\mathbf{w}^T \\mathbf{x} + b) $$</p><h3 id="缺陷" tabindex="-1">缺陷 <a class="header-anchor" href="#缺陷" aria-label="Permalink to &quot;缺陷&quot;">​</a></h3><blockquote><p>异或问题 线性不可分 能实现与 或 非</p></blockquote><p><img src="'+i+'" alt="image-20250115195754940" loading="lazy"></p><h3 id="解决方案" tabindex="-1">解决方案 <a class="header-anchor" href="#解决方案" aria-label="Permalink to &quot;解决方案&quot;">​</a></h3><p>升维（kernel 核方法）</p><p>叠加感知机</p><h2 id="神经网络-1" tabindex="-1">神经网络 <a class="header-anchor" href="#神经网络-1" aria-label="Permalink to &quot;神经网络&quot;">​</a></h2><p>全神经网络 前馈神经网络 循环神经网络</p><p>全神经网络计算量大，用梯度下降算法 随机梯度下降算法 卷积和池化降维</p><p>只训练一种数据，无法夹逼一条直线，用 sigmoid 激活函数</p><h2 id="损失函数" tabindex="-1">损失函数 <a class="header-anchor" href="#损失函数" aria-label="Permalink to &quot;损失函数&quot;">​</a></h2><p>比较俩个概率模型的差别 $$ L(\\theta) = \\frac{1}{N} \\sum_{i=1}^{N} L(y^{(i)}, \\hat{y}^{(i)}) $$ <strong>最小二乘法</strong> 不适合用梯度下降算法 $$ L(\\theta) = \\frac{1}{N} \\sum_{i=1}^{N} \\left( \\hat{y}^{(i)} - y^{(i)} \\right)^2 $$ <strong>极大似然估计法</strong> $$ L(\\theta) = - \\sum_{i=1}^{N} \\left[ y^{(i)} \\log(\\hat{y}^{(i)}) + (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)}) \\right] $$ 挑出似然值最大的概率模型</p><p><strong>交叉熵法</strong> $$ L(\\theta) = - \\sum_{i=1}^{N} \\left[ y^{(i)} \\log(\\hat{y}^{(i)}) + (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)}) \\right] $$</p><h2 id="梯度下降法" tabindex="-1">梯度下降法 <a class="header-anchor" href="#梯度下降法" aria-label="Permalink to &quot;梯度下降法&quot;">​</a></h2><p><strong>前言</strong></p><p>通过<strong>反向传播</strong>调整神经网络参数的策略，通过梯度信息反向传播信息</p><p>没有梯度下降法就没有神经网络，训练量极大，梯度下降法对各个参数，根据贡献大小进行分配偏差，贡献大的多调整，贡献小的少调整。分配方法是向量（梯度的方向）的加法</p><h3 id="什么是梯度" tabindex="-1">什么是梯度 <a class="header-anchor" href="#什么是梯度" aria-label="Permalink to &quot;什么是梯度&quot;">​</a></h3><p>梯度就是函数值增加最快的方向 $$ \\nabla f(x_1, x_2, \\dots, x_n) = \\left( \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\dots, \\frac{\\partial f}{\\partial x_n} \\right) $$ <img src="'+o+'" alt="image-20250116203830055" loading="lazy"></p><p>梯度在神经网络中可以帮助参数确定变化最快的方向</p><h3 id="正向传播" tabindex="-1">正向传播 <a class="header-anchor" href="#正向传播" aria-label="Permalink to &quot;正向传播&quot;">​</a></h3><p>$$ \\mathbf{z}^{(l)} = \\mathbf{W}^{(l)} \\mathbf{a}^{(l-1)} + \\mathbf{b}^{(l)} $$</p><p>$$ \\mathbf{a}^{(l)} = \\sigma(\\mathbf{z}^{(l)}) $$</p><h3 id="反向传播" tabindex="-1">反向传播 <a class="header-anchor" href="#反向传播" aria-label="Permalink to &quot;反向传播&quot;">​</a></h3><ul><li>输出层的梯度</li></ul><p>假设输出层的激活值为 a(L)（L是最后一层），损失函数为 L(θ)，输出层的梯度为： $$ \\delta^{(L)} = \\frac{\\partial L}{\\partial a^{(L)}} \\cdot f&#39;(z^{(L)}) $$ 等号前是 输出层的误差项</p><p>点乘前是 损失函数 L 相对与 输出层激活值 a 的梯度</p><p>点乘后是 激活函数 f 对 加权和 z 的导数</p><ul><li>隐藏层的梯度</li></ul><p>对于每一层 l，从输出层反向传播误差，更新每一层的梯度 $$ \\delta^{(l)} = \\left( W^{(l+1)T} \\delta^{(l+1)} \\right) \\cdot f&#39;(z^{(l)}) $$</p><h3 id="更新参数" tabindex="-1">更新参数 <a class="header-anchor" href="#更新参数" aria-label="Permalink to &quot;更新参数&quot;">​</a></h3><p>计算权重和偏置的梯度</p><ul><li>权重的梯度</li></ul><p>$$ \\frac{\\partial L}{\\partial W^{(l)}} = \\delta^{(l)} \\cdot (a^{(l-1)})^T $$</p><ul><li>偏置的梯度</li></ul><p>$$ \\frac{\\partial L}{\\partial b^{(l)}} = \\delta^{(l)} $$</p><p>更新参数 $$ W^{(L)} = W^{(L)} - \\eta \\frac{\\partial L}{\\partial W^{(L)}} $$</p><p>$$ b^{(L)} = b^{(L)} - \\eta \\frac{\\partial L}{\\partial b^{(L)}} $$</p><p>其中 η 是学习率</p><h2 id="softmax" tabindex="-1">Softmax <a class="header-anchor" href="#softmax" aria-label="Permalink to &quot;Softmax&quot;">​</a></h2><p><strong>前言</strong></p><p>ReLU 防止梯度丢失，放在隐藏层</p><p>sigmoid 适用于打标签</p><p>softmax 适用于分类</p>',54)]))}const b=t(p,[["render",h]]);export{m as __pageData,b as default};
