import{_ as l,c as i,a2 as e,o as t}from"./chunks/framework.BQmytedh.js";const o="/assets/image-20250409185509373.4EwFCuZN.png",r="/assets/image-20250409185918821.BafNK_c5.png",b=JSON.parse('{"title":"AI 大模型应用","description":"","frontmatter":{},"headers":[],"relativePath":"labnote/python/AIapp.md","filePath":"labnote/python/AIapp.md"}'),n={name:"labnote/python/AIapp.md"};function h(p,a,s,d,u,c){return t(),i("div",null,a[0]||(a[0]=[e('<h1 id="ai-大模型应用" tabindex="-1">AI 大模型应用 <a class="header-anchor" href="#ai-大模型应用" aria-label="Permalink to &quot;AI 大模型应用&quot;">​</a></h1><h2 id="rag" tabindex="-1">RAG <a class="header-anchor" href="#rag" aria-label="Permalink to &quot;RAG&quot;">​</a></h2><p>RAG（Retrieval Augmented Generation，检索增强生成）是一种结合信息检索和生成式模型的技术方案。其主要流程包括两个核心环节：</p><ol><li><p><strong>检索（Retrieval）</strong>：基于用户的输入，从外部知识库中检索与查询相关的文本片段，通常使用向量化表示和向量数据库进行语义匹配。</p></li><li><p><strong>生成（Generation）</strong>：将用户查询与检索到的内容作为上下文输入给生成模型（如 GPT 等），由模型输出最终回答。</p></li></ol><p>即我们在本地检索到相关的内容，把它增强到提示词里，然后再去做结果生成。</p><p>简单来说就是利用外部知识动态补充模型生成能力，既能保证回答的准确性，又能在知识库更新时及时反映最新信息（还有一点就是部分业务是内部文档，网上没有，因此可以本地提供知识库来增强 AI 的知识）。</p><blockquote><p>为什么会有 RAG？</p></blockquote><p>因为，随着自然语言处理技术的发展，纯生成模型虽然可以生成流畅的文本，但有时知识会滞后或不够精准。</p><p>通过引入检索模块，RAG 模型能够从外部知识库中提取实时且丰富的信息，再经过生成模型综合处理，提升回答的准确率和覆盖面，而无需重新训练模型。</p><p><img src="'+o+'" alt="image-20250409185509373" loading="lazy"></p><h3 id="rerank" tabindex="-1">ReRank <a class="header-anchor" href="#rerank" aria-label="Permalink to &quot;ReRank&quot;">​</a></h3><p>在 RAG 中，Rerank 是一个对初步检索返回的候选文档列表进行再次排序的过程。</p><p>因为初步检索需要快速地在海量的文档中找出大致相关的文档，其需要考虑效率，所以查找出的文档不会非常准确，这步是粗排。</p><p>在已经筛选的相关的文档中再进行精筛，找出匹配度更高的文档让其排在前面，选其中的 Top-K 然后扔给大模型，提高答案的准确性，这就是 Rerank，也是精排。</p><h3 id="rerank-需要怎么做" tabindex="-1">Rerank 需要怎么做？ <a class="header-anchor" href="#rerank-需要怎么做" aria-label="Permalink to &quot;Rerank 需要怎么做？&quot;">​</a></h3><ol><li><strong>初步检索生成候选文档</strong>：使用速度较快的传统检索方法获得一组候选文档。</li><li><strong>根据 Rerank 模型重新排序</strong>：根据 Rerank 模型匹配得分对候选文档进行排序，选出最相关的 Top-K 文档。</li><li><strong>交给生成模块</strong>：Top-K 候选文档传递给大模型，帮助生成更精准、更富信息量的回答。</li></ol><h3 id="文档问答系统" tabindex="-1">文档问答系统 <a class="header-anchor" href="#文档问答系统" aria-label="Permalink to &quot;文档问答系统&quot;">​</a></h3><p>使用 LangChain 构建文档问答系统主要包含两个核心阶段:</p><ol><li>文档索引阶段(Indexing)</li></ol><p>数据准备过程包含三个主要步骤:</p><ul><li>文档加载：使用 Document Loader 加载各种格式的文档。</li><li>文档分块：使用 Text Splitter 将文档切分成合适大小的块。</li><li>向量化存储：为文本块创建向量嵌入(embeddings)并存储到向量数据库中。</li></ul><ol start="2"><li>问答检索阶段(Retrieval &amp; Generation)</li></ol><p>问答生成过程包含两个主要步骤:</p><ul><li>相关文档检索：根据用户问题从向量库中检索最相关的文档片段。</li><li>答案生成：将检索到的文档和用户问题一起传给 LLM 生成最终答案。</li></ul><h2 id="微调" tabindex="-1">微调 <a class="header-anchor" href="#微调" aria-label="Permalink to &quot;微调&quot;">​</a></h2><p><img src="'+r+'" alt="image-20250409185918821" loading="lazy"></p><h2 id="向量数据库" tabindex="-1">向量数据库 <a class="header-anchor" href="#向量数据库" aria-label="Permalink to &quot;向量数据库&quot;">​</a></h2><p>向量数据库是一种专门设计用来存储和管理向量嵌入(vector embeddings)的数据库系统。它可以将非结构化数据(如文本、图片、音频等)转换成高维向量的形式进行存储，并提供高效的相似性搜索功能。</p><p>在基于大模型的应用开发中，向量数据库主要解决以下核心问题：</p><ol><li>高效的相似性搜索</li></ol><p>通过将用户查询转换为向量，可以快速找到语义相似的内容，这对于实现智能问答、推荐系统等功能至关重要。</p><ol start="2"><li>海量数据处理</li></ol><p>能够高效处理大模型生成的海量数据，传统数据库难以处理百万甚至数十亿的数据点，而向量数据库专门针对这种场景进行了优化。</p><ol start="3"><li>实时交互支持</li></ol><p>在需要实时用户交互的应用中(如聊天机器人)，向量数据库可以确保快速检索相关上下文信息，提供实时响应。</p><h2 id="langchain-中-chain-和-agent" tabindex="-1">LangChain 中 chain 和 agent <a class="header-anchor" href="#langchain-中-chain-和-agent" aria-label="Permalink to &quot;LangChain 中 chain 和 agent&quot;">​</a></h2><p>LangChain框架中的Chain和Agent是两个核心组件，它们的主要区别在于执行方式：</p><ol><li>Chain（链）</li></ol><p>Chain是一个预定义的固定操作序列，按照既定的顺序执行任务。类似于一条生产线，每个步骤都是预先设定好的，按部就班地执行。</p><ol start="2"><li>Agent（代理）</li></ol><p>Agent则是一个能够动态决策的智能体，它可以根据当前情况自主选择使用什么工具、采取什么行动来完成任务。Agent使用语言模型作为推理引擎，能够实时判断和选择最优的行动序列。</p><h3 id="多路召回的动态权重分配" tabindex="-1">多路召回的动态权重分配 <a class="header-anchor" href="#多路召回的动态权重分配" aria-label="Permalink to &quot;多路召回的动态权重分配&quot;">​</a></h3><p>在 LangChain.js 中实现多路召回的动态权重分配主要有以下几种方式:</p><ol><li>使用 EnsembleRetriever</li></ol><p>EnsembleRetriever 是 LangChain 提供的一个组合检索器，可以将多个检索器的结果进行组合。它支持不同的权重分配策略。</p><ol start="2"><li>自定义权重分配逻辑</li></ol><p>可以通过实现自定义的权重分配逻辑，根据查询内容或其他因素动态调整不同检索器的权重。</p><ol start="3"><li>查询分析路由</li></ol><p>通过查询分析来选择最合适的检索器或调整权重分配。</p><h2 id="大模型涌现能力" tabindex="-1">大模型涌现能力 <a class="header-anchor" href="#大模型涌现能力" aria-label="Permalink to &quot;大模型涌现能力&quot;">​</a></h2><p>大模型的涌现能力（Emergent Abilities）是指在模型规模达到某个临界值后突然出现的、在小规模模型中不存在的能力。这种能力的出现往往是不可预测的，无法通过简单外推小模型的性能来预测。</p><ol><li>思维链推理（Chain-of-thought）</li></ol><p>这是一种在解决复杂问题时能够像人类一样展示推理过程的能力。当模型规模达到一定程度（约 10^22 FLOPs）后，模型能够生成中间推理步骤，大幅提升在数学应用题等多步骤推理任务上的表现。</p><ol start="2"><li>指令跟随能力</li></ol><p>模型能够准确理解和执行自然语言指令，无需像小模型那样依赖大量示例。这种能力在大规模模型中表现得更为突出，能够理解更复杂的指令并产生符合要求的输出。</p><ol start="3"><li>多任务理解与迁移</li></ol><p>大模型展现出了强大的跨任务泛化能力，能够在没有专门训练的情况下完成新的任务。比如在 MMLU（Massive Multi-task Language Understanding）测试中，大模型可以回答涉及数学、历史、法律等多个领域的问题。</p><h2 id="突破大模型上下文限制" tabindex="-1">突破大模型上下文限制 <a class="header-anchor" href="#突破大模型上下文限制" aria-label="Permalink to &quot;突破大模型上下文限制&quot;">​</a></h2><ol><li>文本压缩与分段处理</li></ol><ul><li>文本预处理：使用中间截断法(middle truncation)对过长文本进行压缩。</li><li>智能分段：将长文本按语义完整性分成多个片段，保持上下文连贯性。</li><li>动态窗口：根据查询需求动态调整上下文窗口大小，实现查询感知的上下文化处理。</li></ul><ol start="2"><li>检索增强生成(RAG)</li></ol><ul><li>向量检索：使用向量数据库存储文档片段，按相关性检索。</li><li>混合检索：结合关键词和语义检索，提高检索准确性。</li><li>动态更新：支持知识库的实时更新，确保信息时效性。</li></ul><ol start="3"><li>模型优化方案</li></ol><ul><li>Ring Attention：通过改进注意力机制提升计算效率。</li><li>相对位置编码：采用相对位置编码替代绝对位置编码。</li><li>模型微调：针对长文本场景进行特定任务微调。</li></ul><h2 id="ai-幻觉" tabindex="-1">AI 幻觉 <a class="header-anchor" href="#ai-幻觉" aria-label="Permalink to &quot;AI 幻觉&quot;">​</a></h2><h3 id="rag-1" tabindex="-1">RAG <a class="header-anchor" href="#rag-1" aria-label="Permalink to &quot;RAG&quot;">​</a></h3><h3 id="提示工程与边界设置" tabindex="-1">提示工程与边界设置 <a class="header-anchor" href="#提示工程与边界设置" aria-label="Permalink to &quot;提示工程与边界设置&quot;">​</a></h3><h3 id="知识图谱增强" tabindex="-1">知识图谱增强 <a class="header-anchor" href="#知识图谱增强" aria-label="Permalink to &quot;知识图谱增强&quot;">​</a></h3><h3 id="多模态融合与交叉验证" tabindex="-1">多模态融合与交叉验证 <a class="header-anchor" href="#多模态融合与交叉验证" aria-label="Permalink to &quot;多模态融合与交叉验证&quot;">​</a></h3><h2 id="rag-召回结果不满意" tabindex="-1">RAG 召回结果不满意 <a class="header-anchor" href="#rag-召回结果不满意" aria-label="Permalink to &quot;RAG 召回结果不满意&quot;">​</a></h2><p>当 RAG 系统的召回结果与用户查询意图不匹配时,我们可以从以下三个主要环节进行优化:</p><ol><li>预处理阶段优化</li></ol><ul><li>数据清洗：删除无关文本、特殊字符和噪声数据,纠正拼写和语法错误。</li><li>元数据增强：为文档添加概念标签、层级信息等元数据,提升检索效率。</li><li>分块策略：根据具体任务调整chunk大小,确保信息完整性和相关性。</li></ul><ol start="2"><li>检索阶段优化</li></ol><ul><li>混合检索：结合关键词检索(BM25)和语义检索(向量检索)的优势。</li><li>查询重写：使用 LLM 从不同角度重写用户查询,生成多个查询变体。</li><li>重排序：对检索结果进行二次排序,提升最相关文档的排名。</li></ul><ol start="3"><li>后处理阶段优化</li></ol><ul><li>上下文压缩：压缩和过滤不相关的上下文内容。</li><li>RAG Fusion：结合多查询检索和文档重排序,提高检索准确性。</li></ul>',77)]))}const m=l(n,[["render",h]]);export{b as __pageData,m as default};
