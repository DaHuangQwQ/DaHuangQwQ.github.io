import{_ as e,c as r,a2 as n,o as a}from"./chunks/framework.BQmytedh.js";const l=JSON.parse('{"title":"Finetune","description":"","frontmatter":{},"headers":[],"relativePath":"labnote/python/finetune.md","filePath":"labnote/python/finetune.md"}'),d={name:"labnote/python/finetune.md"};function o(i,t,s,h,p,b){return a(),r("div",null,t[0]||(t[0]=[n('<h1 id="finetune" tabindex="-1">Finetune <a class="header-anchor" href="#finetune" aria-label="Permalink to &quot;Finetune&quot;">​</a></h1><p>注意力机制解决了长距离依赖问题和梯度消失问题，关注重点，提高了并行计算的效率</p><p>统计语言模型</p><p>神经网络语言模型</p><table tabindex="0"><thead><tr><th>阶段</th><th>定义与历史背景</th><th>里程碑成果</th><th>限制</th></tr></thead><tbody><tr><td>统计语言模型</td><td>初期的语言模型，依赖于统计分布和频率来预测单词。起源于信息论和早期计算机科学。</td><td>1. 统计机器翻译<br>2. n-gram模型和HMM<br>3. 最大熵模型</td><td>1. 难以捕捉长期依赖性<br>2. 需要复杂的特征工程</td></tr><tr><td>神经网络语言模型</td><td>神经网络在捕捉复杂模式方面的优势使其成为语言模型的一个进步。包括RNN和LSTM在内的模型被用于捕捉序列数据中的依赖关系。</td><td>1. 神经网络语言模型（NNLM）<br>2. RNN处理序列数据<br>3. LSTM在语言模型中的应用</td><td>1. 计算效率和扩展性有限<br>2. 梯度消失或爆炸问题</td></tr><tr><td>基于Transformer大语言模型</td><td>Transformer架构通过自注意力机制改进了长距离依赖问题的处理，并提高了并行化处理的效率。</td><td>1. Transformer架构<br>2. BERT 模型<br>3. GPT 模型家族</td><td>1. 计算资源消耗<br>2. 可能放大数据偏见</td></tr></tbody></table>',5)]))}const u=e(d,[["render",o]]);export{l as __pageData,u as default};
