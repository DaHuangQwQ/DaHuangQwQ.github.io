import{_ as d,c as r,a2 as e,o as a}from"./chunks/framework.BQmytedh.js";const p=JSON.parse('{"title":"Finetune","description":"","frontmatter":{},"headers":[],"relativePath":"labnote/python/finetune.md","filePath":"labnote/python/finetune.md"}'),n={name:"labnote/python/finetune.md"};function o(i,t,h,b,s,l){return a(),r("div",null,t[0]||(t[0]=[e('<h1 id="finetune" tabindex="-1">Finetune <a class="header-anchor" href="#finetune" aria-label="Permalink to &quot;Finetune&quot;">​</a></h1><p>注意力机制解决了长距离依赖问题和梯度消失问题，关注重点，提高了并行计算的效率</p><p>统计语言模型</p><p>神经网络语言模型</p><table tabindex="0"><thead><tr><th>阶段</th><th>定义与历史背景</th><th>里程碑成果</th><th>限制</th></tr></thead><tbody><tr><td>统计语言模型</td><td>初期的语言模型，依赖于统计分布和频率来预测单词。起源于信息论和早期计算机科学。</td><td>1. 统计机器翻译<br>2. n-gram模型和HMM<br>3. 最大熵模型</td><td>1. 难以捕捉长期依赖性<br>2. 需要复杂的特征工程</td></tr><tr><td>神经网络语言模型</td><td>神经网络在捕捉复杂模式方面的优势使其成为语言模型的一个进步。包括RNN和LSTM在内的模型被用于捕捉序列数据中的依赖关系。</td><td>1. 神经网络语言模型（NNLM）<br>2. RNN处理序列数据<br>3. LSTM在语言模型中的应用</td><td>1. 计算效率和扩展性有限<br>2. 梯度消失或爆炸问题</td></tr><tr><td>基于Transformer大语言模型</td><td>Transformer架构通过自注意力机制改进了长距离依赖问题的处理，并提高了并行化处理的效率。</td><td>1. Transformer架构<br>2. BERT 模型<br>3. GPT 模型家族</td><td>1. 计算资源消耗<br>2. 可能放大数据偏见</td></tr></tbody></table><table tabindex="0"><thead><tr><th>特性</th><th>BERT</th><th>GPT</th></tr></thead><tbody><tr><td>训练方式</td><td>自编码（Autoencoding）</td><td>自回归（Autoregressive）</td></tr><tr><td>预测目标</td><td>给定上下文，预测其中的一个或多个缺失单词</td><td>在给定前面的单词时，预测下一个单词</td></tr><tr><td>输入处理</td><td>双向，可以同时考虑一个词的左右上下文</td><td>单向（从左到右或者从右到左）</td></tr><tr><td>适用场景</td><td>适合理解上下文，有助于信息提取、问答系统、情感分析等</td><td>适合生成式任务，如文章生成、诗歌创作等</td></tr><tr><td>架构</td><td>基于Transformer的编码器</td><td>基于Transformer的解码器</td></tr><tr><td>语言模型</td><td>判别式（Discriminative）</td><td>生成式（Generative）</td></tr><tr><td>优点</td><td>对上下文理解能力较强</td><td>预测的连贯性较强</td></tr><tr><td>缺点</td><td>生成的文本连贯性较弱</td><td>对上下文理解能力相对较弱</td></tr></tbody></table><h2 id="bert-vs-gpt-共识" tabindex="-1">BERT vs GPT 共识 <a class="header-anchor" href="#bert-vs-gpt-共识" aria-label="Permalink to &quot;BERT vs GPT 共识&quot;">​</a></h2><table tabindex="0"><thead><tr><th>特性</th><th>描述</th></tr></thead><tbody><tr><td>模型架构</td><td>Transformer</td></tr><tr><td>数据预处理</td><td>都需要对数据进行Tokenization，一般使用词片方法（Subword Tokenization）</td></tr><tr><td>模型训练</td><td>均使用了大量的无标签数据进行预训练</td></tr><tr><td>任务迁移</td><td>都可以通过Fine-tuning方式进行任务迁移</td></tr><tr><td>训练目标</td><td>都试图通过预训练理解语言的一般模式，如语法、语义、上下文关系等</td></tr><tr><td>多语言支持</td><td>均支持多语言模型训练</td></tr></tbody></table>',8)]))}const u=d(n,[["render",o]]);export{p as __pageData,u as default};
